{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wired-trail",
   "metadata": {},
   "source": [
    "---\n",
    "# Train/Test Split Modeling\n",
    "\n",
    "In this section I will be using estimators to create prediction models to predict which text relates to which indicators. \n",
    "\n",
    "To be able to use find model accuracy I will need to split the train data into train and test tests to find the best model through calculating the accuracy and hamming score which can be use if the prediction have a true data to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sklearn\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-eight",
   "metadata": {},
   "source": [
    "### Spliting the Train Data into Train and Test Sets\n",
    "\n",
    "To be able to use the accuracy and hamming function we will need to split the train data in to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.read_csv('data/clean_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "text = train_clean.shape[0]\n",
    "print(f'There are {text} Train Text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_clean[['clean_text']]\n",
    "y = train_clean[['3.1.1', '3.1.2', '3.2.1', '3.2.2','3.3.1', '3.3.2', '3.3.3', '3.3.4', '3.3.5', '3.4.1', '3.4.2', '3.5.1','3.5.2', '3.6.1', '3.7.1', '3.7.2', '3.8.1', '3.8.2', '3.9.1', '3.9.2','3.9.3', '3.a.1', '3.b.1', '3.b.2', '3.b.3', '3.c.1', '3.d.1']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_Xtrain=[]\n",
    "clean_text_Xtest=[]\n",
    "\n",
    "for text in X_train['clean_text']:\n",
    "    clean_text_Xtrain.append(text)\n",
    "    \n",
    "\n",
    "for text in X_test['clean_text']:\n",
    "    clean_text_Xtest.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_cvec_data_features = cvec.fit_transform(clean_text_Xtrain)\n",
    "Xtest_cvec_data_features = cvec.transform(clean_text_Xtest)\n",
    "print('XTrain cvec Data Shape',Xtrain_cvec_data_features.shape)\n",
    "print('XTest cvec Data Shape',Xtest_cvec_data_features.shape)\n",
    "        \n",
    "Xtrain_tvec_data_features = tvec.fit_transform(clean_text_Xtrain)\n",
    "Xtest_tvec_data_features = tvec.transform(clean_text_Xtest)\n",
    "print('XTrain tvec Data Shape',Xtrain_tvec_data_features.shape)\n",
    "print('XTest tvec Data Shape',Xtest_tvec_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yTrain Shape',y_train.shape)\n",
    "print('yTest Shape',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-malawi",
   "metadata": {},
   "source": [
    "### The Hamming Score function\n",
    "\n",
    "Hamming-Loss is the fraction of labels that are incorrectly predicted, i.e., the fraction of the wrong labels to the total number of labels.\n",
    "\n",
    "It reports how many times on average, the relevance of an example to a class label is incorrectly predicted. Therefore, hamming loss takes into account the prediction error (an incorrect label is predicted) and missing error (a relevant label not predicted), normalized over total number of classes and total number of examples.\n",
    "\n",
    "We would expect the hamming loss to be 0, which would imply no error. This means practically the smaller the value of hamming loss, the better the performance of the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hamming loss function\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    temp=0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += np.size(y_true[i] == y_pred[i]) - np.count_nonzero(y_true[i] == y_pred[i])\n",
    "    return temp/(y_true.shape[0] * y_true.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-sewing",
   "metadata": {},
   "source": [
    "### Exact Match Ratio or Subset Accuracy\n",
    "The Exact Match Ratio or Subset Accuracy which is the most strict metric, indicating the percentage of samples that have all their labels classified correctly.\n",
    "\n",
    "The disadvantage of this measure is that multi-class classification problems have a chance of being partially correct, but here we ignore those partially correct matches.\n",
    "\n",
    "There is also function in scikit-learn which implements subset accuracy, called as accuracy_score.\n",
    "\n",
    "Now we will start modeling and get the accuracy for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-proposition",
   "metadata": {},
   "source": [
    "### Linear Regression (Base Model)\n",
    "\n",
    "The base model produced Hamming Loss at 1.0 and Accuracy at 0.072.\n",
    "\n",
    "** Due to long run time cvec has been commented **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "#lr = LinearRegression()\n",
    "#lr.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "#prediction = lr.predict(Xtest_cvec_data_features)\n",
    "\n",
    "#print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "#print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "#lr = LinearRegression()\n",
    "#lr.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "#prediction = lr.predict(Xtest_tvec_data_features)\n",
    "\n",
    "#print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "#print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-wilderness",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "prediction = knn.predict(Xtest_cvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "prediction = knn.predict(Xtest_tvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-river",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some error might occur when running so ths code block will be commented out\n",
    "#Below show the accuracy and hamming loss score of the code block\n",
    "#cvec using MultiOutputClassifier\n",
    "#log = MultiOutputClassifier(LogisticRegression(solver='lbfgs'))\n",
    "#log.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "#prediction = log.predict(Xtest_cvec_data_features)\n",
    "\n",
    "#print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "#print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))\n",
    "\n",
    "#Prediction Accuracy: 0.3071786310517529\n",
    "#Prediction Hamming Loss Accuracy: 0.05237123601063501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec using MultiOutputClassifier\n",
    "log = MultiOutputClassifier(LogisticRegression(solver='lbfgs'))\n",
    "log.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "prediction = log.predict(Xtest_tvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-proportion",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "prediction = tree.predict(Xtest_cvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "prediction = tree.predict(Xtest_tvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-kelly",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#cvec\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "prediction = mlp.predict(Xtest_cvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "prediction = mlp.predict(Xtest_tvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-validation",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "prediction = rfc.predict(Xtest_cvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "prediction = rfc.predict(Xtest_tvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-elite",
   "metadata": {},
   "source": [
    "### Ridge Classifier CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "#cvec\n",
    "ridge = MultiOutputClassifier(RidgeClassifierCV())\n",
    "ridge.fit(Xtrain_cvec_data_features,y_train)\n",
    "\n",
    "prediction = ridge.predict(Xtest_cvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "ridge = MultiOutputClassifier(RidgeClassifierCV())\n",
    "ridge.fit(Xtrain_tvec_data_features,y_train)\n",
    "\n",
    "prediction = ridge.predict(Xtest_tvec_data_features)\n",
    "\n",
    "print('Prediction Accuracy:',accuracy_score(y_test.to_numpy(),prediction))\n",
    "print('Prediction Hamming Loss Accuracy:',hamming_loss(prediction,y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-symbol",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling on Actual Train and Test Data\n",
    "\n",
    "In this section I will be using estimators to create prediction models to predict which text relates to which indicators. This will be testing on unseen occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.read_csv('data/clean_train.csv')\n",
    "test_clean = pd.read_csv('data/clean_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-value",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "text = train_clean.shape[0]\n",
    "print(f'There are {text} Train Text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "text = test_clean.shape[0]\n",
    "print(f'There are {text} Test Text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_train=[]\n",
    "clean_text_test=[]\n",
    "\n",
    "for text in train_clean['clean_text']:\n",
    "    clean_text_train.append(text)\n",
    "    \n",
    "\n",
    "for text in test_clean['clean_text']:\n",
    "    clean_text_test.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cvec_data_features = cvec.fit_transform(clean_text_train)\n",
    "test_cvec_data_features = cvec.transform(clean_text_test)\n",
    "print('Train cvec Data Shape',train_cvec_data_features.shape)\n",
    "print('Test cvec Data Shape',test_cvec_data_features.shape)\n",
    "        \n",
    "train_tvec_data_features = tvec.fit_transform(clean_text_train)\n",
    "test_tvec_data_features = tvec.transform(clean_text_test)\n",
    "print('Train tvec Data Shape',train_tvec_data_features.shape)\n",
    "print('Test tvec Data Shape',test_tvec_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_clean[['3.1.1', '3.1.2', '3.2.1', '3.2.2','3.3.1', '3.3.2', '3.3.3', '3.3.4', '3.3.5', '3.4.1', '3.4.2', '3.5.1','3.5.2', '3.6.1', '3.7.1', '3.7.2', '3.8.1', '3.8.2', '3.9.1', '3.9.2','3.9.3', '3.a.1', '3.b.1', '3.b.2', '3.b.3', '3.c.1', '3.d.1']]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-logistics",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_cvec_data_features,y)\n",
    "\n",
    "prediction = mlp.predict(test_cvec_data_features)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mlp = pd.DataFrame(prediction)\n",
    "\n",
    "#renaming the columns\n",
    "predict_mlp.columns = y.columns[:]\n",
    "\n",
    "submission_mlp_cvec = pd.concat([test_clean['Unique ID'],predict_mlp],axis=1)\n",
    "submission_mlp_cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_mlp_cvec.to_csv('predictions/submission_mlp_cvec.csv',index=False)\n",
    "#Hamming Loss Score of 0.0488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_tvec_data_features,y)\n",
    "\n",
    "prediction = mlp.predict(test_tvec_data_features)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mlp = pd.DataFrame(prediction)\n",
    "\n",
    "#renaming the columns\n",
    "predict_mlp.columns = y.columns[:]\n",
    "\n",
    "submission_mlp_tvec = pd.concat([test_clean['Unique ID'],predict_mlp],axis=1)\n",
    "submission_mlp_tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_mlp_tvec.to_csv('predictions/submission_mlp_tvec.csv',index=False)\n",
    "#Hamming Loss Score of 0.0464"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-failing",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_cvec_data_features,y)\n",
    "\n",
    "prediction = knn.predict(test_cvec_data_features)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_knn = pd.DataFrame(prediction)\n",
    "\n",
    "#renaming the columns\n",
    "predict_knn.columns = y.columns[:]\n",
    "\n",
    "submission_knn_cvec = pd.concat([test_clean['Unique ID'],predict_knn],axis=1)\n",
    "submission_knn_cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_knn_cvec.to_csv('predictions/submission_knn_cvec.csv',index=False)\n",
    "#Hamming Loss Score of 0.0594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_tvec_data_features,y)\n",
    "\n",
    "prediction = knn.predict(test_tvec_data_features)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_knn = pd.DataFrame(prediction)\n",
    "\n",
    "#renaming the columns\n",
    "predict_knn.columns = y.columns[:]\n",
    "\n",
    "submission_knn_tvec = pd.concat([test_clean['Unique ID'],predict_knn],axis=1)\n",
    "submission_knn_tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_knn_tvec.to_csv('predictions/submission_knn_tvec.csv',index=False)\n",
    "#Hamming Loss Score of 0.0513"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-carnival",
   "metadata": {},
   "source": [
    "### Ridge Classifier CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec\n",
    "ridge = MultiOutputClassifier(RidgeClassifierCV())\n",
    "ridge.fit(train_cvec_data_features,y)\n",
    "\n",
    "prediction = ridge.predict(test_cvec_data_features)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ridge = pd.DataFrame(prediction)\n",
    "\n",
    "#renaming the columns\n",
    "predict_ridge.columns = y.columns[:]\n",
    "\n",
    "submission_ridge_cvec = pd.concat([test_clean['Unique ID'],predict_ridge],axis=1)\n",
    "submission_ridge_cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ridge_cvec.to_csv('predictions/submission_ridge_cvec.csv',index=False)\n",
    "#Hamming Loss Score of 0.0818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvec\n",
    "ridge = MultiOutputClassifier(RidgeClassifierCV())\n",
    "ridge.fit(train_tvec_data_features,y)\n",
    "\n",
    "prediction = ridge.predict(test_tvec_data_features)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ridge = pd.DataFrame(prediction)\n",
    "\n",
    "#renaming the columns\n",
    "predict_ridge.columns = y.columns[:]\n",
    "\n",
    "submission_ridge_tvec = pd.concat([test_clean['Unique ID'],predict_ridge],axis=1)\n",
    "submission_ridge_tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ridge_tvec.to_csv('predictions/submission_ridge_tvec.csv',index=False)\n",
    "#Hamming Loss Score of 0.0447"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-france",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation and Best Model\n",
    " \n",
    "From exploring each model hamming loss score, I was able to indentify the best model for text classification is the Ridge Classification CV model with the TfidfVectorizer and its default parameters. The model recieved the lowest hamming loss score at 0.0447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = MultiOutputClassifier(RidgeClassifierCV())\n",
    "ridge.fit(train_tvec_data_features,y)\n",
    "\n",
    "prediction_prob = ridge.predict_proba(test_tvec_data_features)\n",
    "prediction_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-welding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-tenant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-malpractice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-escape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-bruce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-trouble",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-surgery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-punishment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
